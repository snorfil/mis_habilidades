# -*- coding: utf-8 -*-
"""ExamenPartePractica_setas_primera_convocatoria_carlos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XlBX-pPUahYwuxse0eYZe-MH-upbtjok

# Descripción del problema

En este ejercicio se plantea el predecir la variable **class**, la cual describe si una seta es comestible o no.

## Características

El conjunto de datos principalmente consiste en información sobre setas. Cada fila en el conjunto de datos representa las características de una seta concreta que puede ser clasificada como comestible (edible) o tóxica (poisonous). Esta clasificación se puede ver en la columna **class** con el valor *e* cuando es comestible o *p* cuando sea tóxica.

### Descripción de las características

## Attribute Information

- class: edible=e, poisonous=p
- cap-shape: bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s
- cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s
- cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y
- bruises: bruises=t,no=f
- odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s
- gill-attachment: attached=a,descending=d,free=f,notched=n
- gill-spacing: close=c,crowded=w,distant=d
- gill-size: broad=b,narrow=n
- gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y
- stalk-shape: enlarging=e,tapering=t
- stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?
- stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s
- stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s
- stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
- stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
- veil-type: partial=p,universal=u
- veil-color: brown=n,orange=o,white=w,yellow=y
- ring-number: none=n,one=o,two=t
- ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z
- spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y
- population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y
- habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d
"""

## Cargar el dataset

import pandas as pd
df_train = pd.read_csv('mushrooms.csv', delimiter=',')

df_train

"""# Ejercicios

## Instrucciones

A continuación se presentan una serie de ejercicios que el alumno deberá resolver siguiendo estas instrucciones:

*   NO modificar ninguno de los cuadros de texto del documento.
*   NO crear ningún nuevo cuadro de texto o código.
*   ÚNICAMENTE escribir código dentro de los cuadros de código. Si el alumno tiene que realizar alguna aclaración o descripción deberá hacerla a través de comentarios Python.

# Análisis del dataset (0.5 puntos)

## **(0.2 puntos)** Ejercicio 1. Comprueba a través de un diagrama de barras si la variable a predecir (class) se encuentra balanceada o no.
"""

import pandas as pd
import matplotlib.pyplot as plt

# Carga del dataset
df_train = pd.read_csv('mushrooms.csv', delimiter=',')

# Conteo de las clases
conteo_class = df_train['class'].value_counts()

# Creación del diagrama de barras
plt.figure(figsize=(8, 5))
conteo_class.plot(kind='bar', color=['green', 'red'])
plt.title('Número de Ocurrencias de comestibles y no comestibles')
plt.xlabel('Clase')
plt.ylabel('Número de Ocurrencias')
plt.xticks(rotation=0)
plt.show()

"""## **(0.2 puntos)** Ejercicio 2. Comprueba si alguna de las variables tiene más del 10% de sus ocurrencias con valores nulos."""

# Cálculo del número y porcentaje de valores nulos por columna
missing_values = df_train.isnull().sum()
missing_percentage = (missing_values / len(df_train)) * 100

# Creación de un DataFrame para visualizar los resultados
missing_df = pd.DataFrame({'Numero de nullos': missing_values, 'Porcentaje': missing_percentage})

# Filtrar y ordenar las columnas con valores nulos
missing_df_sorted = missing_df[missing_df > 0].sort_values(by='Porcentaje', ascending=False)

missing_df_sorted

"""## **(0.1 puntos)** Ejercicio 3. Comprueba qué valores diferentes existen para la variable **habitat** y comprueba que corresponde con su descripción en el enunciado del ejercicio"""

# Obtención de los valores únicos para la variable 'habitat'
unique_habitats = df_train['habitat'].unique()

# Mostrar los valores únicos
unique_habitats

"""# Preprocesamiento del dataset (2.5 puntos)

## **(0.2 puntos)** Ejercicio 4. Crea un transformador que elimine un conjunto de columnas pasadas por parámetro.
"""

from sklearn.base import BaseEstimator, TransformerMixin

# Definición del transformador para eliminar columnas
class EliminarColumnas(BaseEstimator, TransformerMixin):
    def __init__(self, columns):
        self.columns = columns

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # Verificar primero si las columnas existen antes de intentar eliminarlas
        cols_to_drop = [col for col in self.columns if col in X.columns]
        return X.drop(cols_to_drop, axis=1, errors='ignore')

"""## **(0.2 puntos)** Ejercicio 5. Transformar todas las variables categóricas (**excepto class**) en numéricas utilizando la técnica de one-hot


"""

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

# Seleccionar todas las columnas categóricas excepto 'class'
columnas_categoricas = df_train.select_dtypes(include='object').columns.drop('class')

# Crear un pipeline que aplique One-Hot Encoding a las columnas categóricas
pipeline_categorico = Pipeline([
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Crear un transformador de columnas para aplicar el pipeline solo a las columnas categóricas
preprocessor = ColumnTransformer([
    ('cat', pipeline_categorico, columnas_categoricas)
])

# Aplicar el preprocesador al DataFrame (excepto a la columna 'class')
one_hot_data = preprocessor.fit_transform(df_train.drop('class', axis=1))

one_hot_data

"""## **(0.2 puntos)** Ejercicio 6. Convertir los valores **e** de *class* en 0 y los valores **p** en 1"""

# Función para convertir los valores de 'class'
def conversor(valor):
    if valor == 'p':
        return 1
    else:
        return 0

# Aplicar la función de conversión y guardar los resultados
df_class = df_train['class'].apply(conversor)
df_class

"""## **(0.2 puntos)** Ejercicio 7. Crea un pipeline que agrupe todos los transformadores que se te han pedido. Dicho pipeline tienes que configurarlo para que elimine las columnas "veil-type","gill-color","bruises","ring-type"
"""

from sklearn.pipeline import Pipeline

# Crear un pipeline completo incluyendo la eliminación de columnas y el preprocesamiento categórico
full_pipeline = Pipeline([
    ('eliminar_columnas', EliminarColumnas(columns=['veil-type', 'gill-color', 'bruises', 'ring-type'])),
    ('preprocesador', preprocessor),
    ('clasificador', None)  # Aquí se podría añadir un clasificador
])

# Aplicar el pipeline completo al dataset de entrenamiento
# Nota: Aún no se ha especificado el clasificador en el pipeline
processed_data = full_pipeline.fit_transform(df_train)

"""## Ejercicio 8. Sobre la correlación de las variables (1 punto)

### **(0.25 puntos)** 8.1. Muestra la matriz de correlación de todas las variables del dataset
"""

import seaborn as sns

# Suponiendo que las características han sido ya codificadas como numéricas
# Convertir a DataFrame para visualización
processed_df = pd.DataFrame(processed_data)

# Calcular la matriz de correlación
correlation_matrix = processed_df.corr()

# Visualización de la matriz de correlación con Seaborn
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, fmt=".2f")
plt.title('Matriz de Correlación')
plt.show()

"""### **(0.75 puntos)** 8.2  Modifica el dataset para quedarte con las variables que tengan una correlación mayor que 0.3"""

# Supongamos que 'y' es la columna de la variable objetivo convertida a numérica previamente
correlation_with_target = processed_df.corrwith(df_class)

# Filtrar columnas con una correlación superior a 0.3
high_corr_columns = correlation_with_target[correlation_with_target.abs() > 0.3].index

# Seleccionar solo las columnas filtradas en el DataFrame
high_corr_df = processed_df[high_corr_columns]

"""##  Ejercicio 9. Divide el dataset guardando el 80% para entrenamiento y el 20% para test **(0.2 puntos)**. Posteriormente normaliza ambos subconjuntos **(0.5 puntos)**."""

from sklearn.model_selection import train_test_split

# Asignar las características y la variable objetivo
X = high_corr_df  # Características filtradas por alta correlación
y = df_class      # Variable objetivo ya convertida

# Dividir el dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


ºfrom sklearn.preprocessing import StandardScaler

# Crear un objeto StandardScaler
scaler = StandardScaler()

# Ajustar y transformar los datos de entrenamiento y solo transformar los de prueba
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Creación e interpretación de modelos (3 puntos)

## **(1 punto)** 10.1. Crea una red neuronal con las siguientes características


*   Tiene 4 capas en total. Una de entrada, dos ocultas y una de salida.
*   Debes configurar cada capa con su función de activación correspondiente.
*   Configura cada capa con el número de neuronas correspondientes.
*   Configura el optimipador Adam.
*   Configura como función de coste Sparse Categorical Crossentropy.
*   Entrena el modelo utilizando el 10% del conjunto de entrenamiento para validación.
"""

import tensorflow as tf
from tensorflow import keras

# Definir la estructura del modelo
model = keras.models.Sequential([
    keras.layers.Dense(300, activation="relu", input_shape=[X_train_scaled.shape[1]]),
    keras.layers.Dense(100, activation="relu"),
    keras.layers.Dense(1, activation="sigmoid")  # Cambio a 'sigmoid' porque la clase es binaria
])

# Compilación del modelo
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Entrenamiento del modelo
history = model.fit(X_train_scaled, y_train, epochs=30, validation_split=0.1)

"""## **(0.5 punto)** Ejercicio 10.2. Genera la matriz de confusión de dicho modelo. ¿Cuántas setas que NO son comestibles las ha clasificado como comestibles?"""

from sklearn.metrics import confusion_matrix

# Predecir los valores para el conjunto de prueba
y_pred = model.predict(X_test_scaled)
y_pred = (y_pred > 0.5).astype(int)  # Convertir probabilidades a clases binarias

# Generar la matriz de confusión
conf_mat = confusion_matrix(y_test, y_pred)

# Visualizar la matriz de confusión
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

"""## **(0.5 punto)** Ejercicio 10.3. Genera tres modelos SVM de clasificación utilizando diferentes kernels e hiperparámetros."""

from sklearn.svm import SVC

# Modelos SVM con diferentes kernels
svm_linear = SVC(kernel='linear')
svm_rbf = SVC(kernel='rbf')
svm_poly = SVC(kernel='poly', degree=3)

# Entrenar los modelos
svm_linear.fit(X_train_scaled, y_train)
svm_rbf.fit(X_train_scaled, y_train)
svm_poly.fit(X_train_scaled, y_train)

"""## **(1 punto)** Ejercicio 10.4. Genera un informe de evaluación de los tres modelos generados anteriormente.

Cada informe deberá contener:

*   la exactitud, precisión, y recall del modelo generado.
*   la matriz de confusión del modelo

Finalmente, interpreta los resultados indicando cuál de estos dos modelos y la red neuronal ha dado mejores resultados.


"""

from sklearn.metrics import classification_report

# Función para evaluar cada modelo
def evaluate_model(model, X_test_scaled, y_test):
    y_pred = model.predict(X_test_scaled)
    print(classification_report(y_test, y_pred))
    conf_mat = confusion_matrix(y_test, y_pred)
    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
    plt.show()

# Evaluación de cada modelo SVM
print("Evaluación del modelo SVM con kernel lineal:")
evaluate_model(svm_linear, X_test_scaled, y_test)

print("Evaluación del modelo SVM con kernel RBF:")
evaluate_model(svm_rbf, X_test_scaled, y_test)

print("Evaluación del modelo SVM con kernel polinomial:")
evaluate_model(svm_poly, X_test_scaled, y_test)