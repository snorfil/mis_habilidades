# -*- coding: utf-8 -*-
"""ExamenPartePractica_setas_primera_convocatoria_carlos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XlBX-pPUahYwuxse0eYZe-MH-upbtjok

# Descripción del problema

En este ejercicio se plantea el predecir la variable **class**, la cual describe si una seta es comestible o no.

## Características

El conjunto de datos principalmente consiste en información sobre setas. Cada fila en el conjunto de datos representa las características de una seta concreta que puede ser clasificada como comestible (edible) o tóxica (poisonous). Esta clasificación se puede ver en la columna **class** con el valor *e* cuando es comestible o *p* cuando sea tóxica.

### Descripción de las características

## Attribute Information

- class: edible=e, poisonous=p
- cap-shape: bell=b,conical=c,convex=x,flat=f,knobbed=k,sunken=s
- cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s
- cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y
- bruises: bruises=t,no=f
- odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s
- gill-attachment: attached=a,descending=d,free=f,notched=n
- gill-spacing: close=c,crowded=w,distant=d
- gill-size: broad=b,narrow=n
- gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g,green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y
- stalk-shape: enlarging=e,tapering=t
- stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?
- stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s
- stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s
- stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
- stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y
- veil-type: partial=p,universal=u
- veil-color: brown=n,orange=o,white=w,yellow=y
- ring-number: none=n,one=o,two=t
- ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z
- spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y
- population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y
- habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d
"""

## Cargar el dataset

import pandas as pd

df_train = pd.read_csv('mushrooms.csv', delimiter=',')



import matplotlib.pyplot as plt

# Conteo de las clases
conteo_class = df_train['class'].value_counts()

# Creación del diagrama de barras
plt.figure(figsize=(8, 5))
conteo_class.plot(kind='bar', color=['green', 'red'])
plt.title('Número de Ocurrencias de comestibles y no comestibles')
plt.xlabel('Clase')
plt.ylabel('Número de Ocurrencias')
plt.xticks(rotation=0)
plt.show()

"""## **(0.2 puntos)** Ejercicio 2. Comprueba si alguna de las variables tiene más del 10% de sus ocurrencias con valores nulos."""

# Cálculo del número y porcentaje de valores nulos por columna
null_percentage = df_train.isnull().mean() * 100

# Creación de un DataFrame para visualizar los resultados
# Determinar si alguna columna tiene más del 10% de sus valores como nulos
columns_with_high_nulls = null_percentage[null_percentage > 10]

# Mostrar las columnas que cumplen esta condición
print(columns_with_high_nulls)
# Filtrar y ordenar las columnas con valores nulos


"""## **(0.1 puntos)** Ejercicio 3. Comprueba qué valores diferentes existen para la variable **habitat** y comprueba que corresponde con su descripción en el enunciado del ejercicio"""

# Obtención de los valores únicos para la variable 'habitat'
unique_habitats = df_train['habitat'].unique()

# Valores esperados para 'habitat' según el enunciado
expected_habitats = {'g', 'l', 'm', 'p', 'u', 'w', 'd'}

# Comparar los valores únicos encontrados con los esperados
unexpected_habitats = set(unique_habitats) - expected_habitats
print(unexpected_habitats)

"""# Preprocesamiento del dataset (2.5 puntos)

## **(0.2 puntos)** Ejercicio 4. Crea un transformador que elimine un conjunto de columnas pasadas por parámetro.
"""

from sklearn.base import BaseEstimator, TransformerMixin


# Definición del transformador para eliminar columnas
class EliminarColumnas(BaseEstimator, TransformerMixin):
    def __init__(self, columns):
        self.columns = columns

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        # Eliminar las columnas especificadas
        X_transformed = X.drop(columns=self.columns)
        return X_transformed

# Supongamos que queremos eliminar las columnas 'cap-shape' y 'cap-surface'
columns_to_drop = ['cap-shape', 'cap-surface']
transformer = EliminarColumnas(columns_to_drop)

# Aplicar el transformador a un dataframe
df_transformed = transformer.fit_transform(df_train)

# Mostrar las primeras filas del dataframe transformado
df_transformed.head()

"""## **(0.2 puntos)** Ejercicio 5. Transformar todas las variables categóricas (**excepto class**) en numéricas utilizando la técnica de one-hot


"""

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder

# Seleccionar todas las columnas categóricas
categorical_data = df_train.select_dtypes(include='object')

# Borrando la columna class
columnas_categoricas = categorical_data.drop("class", axis=1).columns.tolist()

# Crear un pipeline que aplique One-Hot Encoding a las columnas categóricas
pipeline_categorico = Pipeline([
    ('onehot', OneHotEncoder())
])


# Crear un transformador de columnas para aplicar el pipeline solo a las columnas categóricas
preprocessor = ColumnTransformer([
    ('cat', OneHotEncoder(), columnas_categoricas)
])

# Aplicar el preprocesador al DataFrame (excepto a la columna 'class')
one_hot_data = preprocessor.fit_transform(df_train)

# Crear un DataFrame con las variables codificadas
df_encoded = pd.DataFrame(one_hot_data.toarray(), columns=preprocessor.get_feature_names_out())

# Calcular la matriz de correlación
correlation_matrix = df_encoded.corr()


import seaborn as sns
# Visualizar la matriz de correlación
plt.figure(figsize=(20, 16))
sns.heatmap(correlation_matrix, cmap='coolwarm')
plt.title('Matriz de Correlación')
plt.show()
"""## **(0.2 puntos)** Ejercicio 6. Convertir los valores **e** de *class* en 0 y los valores **p** en 1"""


# Función para convertir los valores de 'class'
def conversor(valor):
    if valor == 'p':
        return 1
    else:
        return 0


# Aplicar la función de conversión y guardar los resultados
df_class = df_train['class'].apply(conversor)

"""## **(0.2 puntos)** Ejercicio 7. Crea un pipeline que agrupe todos los transformadores que se te han pedido. Dicho pipeline tienes que configurarlo para que elimine las columnas "veil-type","gill-color","bruises","ring-type"
"""

from sklearn.pipeline import Pipeline

# Crear un pipeline completo incluyendo la eliminación de columnas y el preprocesamiento categórico
full_pipeline = Pipeline([
    ('eliminar_columnas', EliminarColumnas(columns=['veil-type', 'gill-color', 'bruises', 'ring-type'])),
])

# Aplicar el pipeline completo al dataset de entrenamiento
# Nota: Aún no se ha especificado el clasificador en el pipeline
processed_data = full_pipeline.fit_transform(df_train)

"""## Ejercicio 8. Sobre la correlación de las variables (1 punto)

### **(0.25 puntos)** 8.1. Muestra la matriz de correlación de todas las variables del dataset
"""

# Aplicar el preprocesador al DataFrame (excepto a la columna 'class')
one_hot_data = preprocessor.fit_transform(df_train)

# Crear un DataFrame con las variables codificadas
df_encoded = pd.DataFrame(one_hot_data.toarray(), columns=preprocessor.get_feature_names_out())

# Calcular la matriz de correlación
correlation_matrix = df_encoded.corr()


import seaborn as sns
# Visualizar la matriz de correlación
plt.figure(figsize=(20, 16))
sns.heatmap(correlation_matrix, cmap='coolwarm')
plt.title('Matriz de Correlación')
plt.show()

"""### **(0.75 puntos)** 8.2  Modifica el dataset para quedarte con las variables que tengan una correlación mayor que 0.3"""

threshold = 0.3
strong_correlation_columns = correlation_matrix.columns[
    (correlation_matrix.abs() > threshold).any(axis=0)]
"""##  Ejercicio 9. Divide el dataset guardando el 80% para entrenamiento y el 20% para test **(0.2 puntos)**. Posteriormente normaliza ambos subconjuntos **(0.5 puntos)**."""

from sklearn.model_selection import train_test_split

# Asignar las características y la variable objetivo
X = correlation_matrix[strong_correlation_columns]  # Características filtradas por alta correlación
y = df_class  # Variable objetivo ya convertida

# Dividir el dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.preprocessing import StandardScaler

# Crear un objeto StandardScaler
scaler = StandardScaler()

# Ajustar y transformar los datos de entrenamiento y solo transformar los de prueba
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

"""# Creación e interpretación de modelos (3 puntos)

## **(1 punto)** 10.1. Crea una red neuronal con las siguientes características


*   Tiene 4 capas en total. Una de entrada, dos ocultas y una de salida.
*   Debes configurar cada capa con su función de activación correspondiente.
*   Configura cada capa con el número de neuronas correspondientes.
*   Configura el optimipador Adam.
*   Configura como función de coste Sparse Categorical Crossentropy.
*   Entrena el modelo utilizando el 10% del conjunto de entrenamiento para validación.
"""
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, InputLayer
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.metrics import SparseCategoricalAccuracy
from sklearn.model_selection import train_test_split

# Separar las características y la etiqueta
X = df_encoded.drop('class_e', axis=1)  # Asumimos que 'class_e' es la columna de etiquetas
y = df_encoded['class_e']

# Dividir los datos en conjuntos de entrenamiento y prueba
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Crear el modelo
model = Sequential([
    InputLayer(input_shape=(X_train.shape[1],)),
    Dense(64, activation='relu'),    # Primera capa oculta
    Dense(32, activation='relu'),    # Segunda capa oculta
    Dense(2, activation='softmax')   # Capa de salida, suponiendo que tenemos dos clases
])

# Compilar el modelo
model.compile(optimizer=Adam(),
              loss=SparseCategoricalCrossentropy(),
              metrics=[SparseCategoricalAccuracy()])

# Entrenar el modelo utilizando el 10% del conjunto de entrenamiento para validación
history = model.fit(X_train, y_train,
                    validation_split=0.1,
                    epochs=20,
                    batch_size=32)

# Evaluar el modelo en el conjunto de prueba
test_loss, test_accuracy = model.evaluate(X_test, y_test)

print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

"""## **(0.5 punto)** Ejercicio 10.2. Genera la matriz de confusión de dicho modelo. ¿Cuántas setas que NO son comestibles las ha clasificado como comestibles?"""

from sklearn.metrics import confusion_matrix

# Predecir los valores para el conjunto de prueba
y_pred = model.predict(X_test_scaled)
y_pred = (y_pred > 0.5).astype(int)  # Convertir probabilidades a clases binarias

# Generar la matriz de confusión
conf_mat = confusion_matrix(y_test, y_pred)

# Visualizar la matriz de confusión
sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()
#
# # Predecir las etiquetas del conjunto de prueba
# y_pred = model.predict(X_test)
# y_pred_classes = y_pred.argmax(axis=1)
#
# # Generar la matriz de confusión
# conf_matrix = confusion_matrix(y_test, y_pred_classes)
# print('Matriz de Confusión:')
# print(conf_matrix)
#
# # Informe de clasificación
# class_report = classification_report(y_test, y_pred_classes, target_names=['No comestible', 'Comestible'])
# print('Informe de Clasificación:')
# print(class_report)
#
# # Mostrar cuántas setas que no son comestibles fueron clasificadas como comestibles
# false_negatives = conf_matrix[0, 1]
# print(f'Número de setas no comestibles clasificadas como comestibles: {false_negatives}')
# """## **(0.5 punto)** Ejercicio 10.3. Genera tres modelos SVM de clasificación utilizando diferentes kernels e hiperparámetros."""

from sklearn.svm import SVC

# Modelos SVM con diferentes kernels
svm_linear = SVC(kernel='linear')
svm_rbf = SVC(kernel='rbf')
svm_poly = SVC(kernel='poly', degree=3)

# Entrenar los modelos
svm_linear.fit(X_train_scaled, y_train)
svm_rbf.fit(X_train_scaled, y_train)
svm_poly.fit(X_train_scaled, y_train)

"""## **(1 punto)** Ejercicio 10.4. Genera un informe de evaluación de los tres modelos generados anteriormente.

Cada informe deberá contener:

*   la exactitud, precisión, y recall del modelo generado.
*   la matriz de confusión del modelo

Finalmente, interpreta los resultados indicando cuál de estos dos modelos y la red neuronal ha dado mejores resultados.


"""

from sklearn.metrics import classification_report


# Función para evaluar cada modelo
def evaluate_model(model, X_test_scaled, y_test):
    y_pred = model.predict(X_test_scaled)
    print(classification_report(y_test, y_pred))
    conf_mat = confusion_matrix(y_test, y_pred)
    sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')
    plt.show()


# Evaluación de cada modelo SVM
print("Evaluación del modelo SVM con kernel lineal:")
evaluate_model(svm_linear, X_test_scaled, y_test)

print("Evaluación del modelo SVM con kernel RBF:")
evaluate_model(svm_rbf, X_test_scaled, y_test)

print("Evaluación del modelo SVM con kernel polinomial:")
evaluate_model(svm_poly, X_test_scaled, y_test)
